import random
import time
import sys
import subprocess
import numpy as np
sys.path.append(sys.path[0] + '/../../')
from path_converter import path_con


def initialization(domain, algNum, instanceIndexFile, optimum_file, metric,
                   solution_checker, initTime, cutoffTime, minTestTimes,
                   logFile):
    # Estimate how many configs can be tested in initTime
    config_pool = []
    full_config_poll = []
    insts = []
    with open(instanceIndexFile, 'r') as f:
        insts = f.read().strip().split('\n')

    n = int(initTime * 40.0 / (float(cutoffTime) * minTestTimes * len(insts)))
    logFile.write('will test %d configs, including the default config\n' % n)

    # read random configs generated by EEAAC to form U
    if domain == 'TSP':
        with open('random_configs_lkh', 'r') as f:
            configs = f.read().strip().split('\n')

        with open('random_configs_lkh_full', 'r') as f:
            full_configs = f.read().strip().split('\n')
        config_zip = zip(configs, full_configs)
    if domain == 'VRPSPDTW':
        with open('random_configs_ga', 'r') as f:
            configs = f.read().strip().split('\n')

        with open('random_configs_ga_full', 'r') as f:
            full_configs = f.read().strip().split('\n')
        config_zip = zip(configs, full_configs)

    config_pool.append(configs[0])
    full_config_poll.append(full_configs[0])

    tmp = zip(*random.sample(config_zip[1:], n-1))
    config_pool.extend(tmp[0])
    full_config_poll.extend(tmp[1])

    # test them
    running_tasks = 0
    test_process = set()
    outDir = path_con('AC_output/GenAS/init/')

    cmd = 'rm %s*' % outDir
    pid = subprocess.Popen(cmd, shell=True)
    pid.communicate()

    for i, config in enumerate(config_pool):
        for j, instance in enumerate(insts):
            for k in range(minTestTimes):
                while True:
                    if running_tasks >= 40:
                        time.sleep(0.1)
                        finishedd = [pid for pid in test_process\
                                     if pid.poll() is not None]
                        test_process -= set(finishedd)
                        running_tasks = len(test_process)
                        continue
                    else:
                        seed = random.randint(0, 1000000)
                        output_file = '%sConfig%d_Ins%d_Seed%d' % (outDir, i, j, k)
                        cmd = 'python '+ path_con('src/util/testing_wrapper.py ')
                        if optimum_file:
                            cmd += '--opt-fn %s ' % optimum_file
                        if metric == 'quality':
                            cmd += '--full-performance '
                        if solution_checker:
                            cmd += '--solution-checker %s ' % solution_checker
                        cmd += '%s %s %s %s %s %s' %\
                                (instance, output_file, cutoffTime,
                                 0, seed, config)

                        test_process.add(subprocess.Popen(cmd, shell=True))
                        running_tasks = len(test_process)
                        break

    # check if subprocess all exits
    while test_process:
        time.sleep(5)
        print 'Still %d testing-instance process not exits' % len(test_process)
        finisheddd = [pidd for pidd in test_process if pidd.poll() is not None]
        test_process -= set(finisheddd)

    # extract testing results
    if metric == 'runtime':
        punish = 10
    elif metric == 'quality':
        punish = 100000

    newFitness = np.zeros((n, len(insts))) * np.nan
    runCount = np.zeros(newFitness.shape) * np.nan
    for i, _ in enumerate(config_pool):
        for j, _ in enumerate(insts):
            for k in range(minTestTimes):
                output_file = '%sConfig%d_Ins%d_Seed%d' % (outDir, i, j, k)
                with open(output_file, 'r') as f:
                    outPut = f.read().strip()
                    values = outPut[outPut.find(':') + 1:].strip().replace(
                        ' ', '').split(',')
                (status, runtime, quality) = (values[0], float(values[1]), float(values[3]))
                if metric == 'runtime' and 'TIMEOUT' in status:
                    runtime = runtime * punish
                if metric == 'quality' and 'TIMEOUT' in status:
                    quality = punish
                if np.isnan(newFitness[i, j]):
                    if metric == 'runtime':
                        newFitness[i, j] = runtime
                    elif metric == 'quality':
                        newFitness[i, j] = quality
                    runCount[i, j] = 1
                else:
                    if metric == 'runtime':
                        newFitness[i, j] += runtime
                    elif metric == 'quality':
                        newFitness[i, j] += quality
                    runCount[i, j] += 1
    newFitness = np.true_divide(newFitness, runCount)
    np.save('%sinitial_fitness' % outDir, newFitness)

    # write to logFile
    logFile.write('Testing initial configs done\n')
    logFile.write(str(newFitness) + '\n')

    selected_index = []
    # greedy selection
    i = 1
    while i <= algNum:
        best_quality = None
        best_index = None
        for j in range(n):
            if j in selected_index:
                continue
            tmp = np.vstack((newFitness[selected_index, :], newFitness[j, :]))
            quality = np.mean(np.min(tmp, axis=0))
            if best_quality is None or quality < best_quality:
                best_quality = quality
                best_index = j
        selected_index.append(best_index)
        logFile.write('select %d config to portoflio, index %d quality %f\n' %\
                      (i, best_index, best_quality))
        i += 1

    portfolio = dict()
    portfolio_fullconfig = dict()

    for i in range(algNum):
        portfolio[i] = config_pool[selected_index[i]]
        portfolio_fullconfig[i] = full_config_poll[selected_index[i]]
    return portfolio, portfolio_fullconfig, newFitness[selected_index, :]
